\chapter{Examples}\label{chapter:examples}




\section{Several examples for the quasilinear case with explicit solutions}

In 2011, Figalli-Kim-McCann \cite{FigalliKimMcCann11} provided a non-negative curvature condition (B3) which is equivalent to the convexity of the 
domain of the objective
 $\pmb \Pi$ or the domain of 
 $\mathcal{L}$ defined below, under some other constraints. One may wonder the question whether this curvature condition (B3) is 
 %equivalent to 
necessary for 
uniqueness of the 
optimizer 
as well. \medskip

According to Loeper \cite{Loeper09}, 
for $c=d^2$, where $d$ is a Riemannian distance, (B3) is satisfied only if the Riemannian sectional curvature is non-negative.
%, some part of the thesis aims to investigate uniqueness without concavity on the hyperbolic spaces with constant negative curvatures. 
This section shows a negative answer to the above question, via uniqueness examples on the hyperbolic spaces with constant negative curvatures%negative curvature spaces
, where (B3) is violated. \medskip

Let $\mathcal{D}$ be a disk with a small radius $\bar{r}$ on $\HH^n$($n\ge 2$), and consider spaces $X= Y = \mathcal{D}$, utility $G(x,y,z)=-\frac{1}{2}d_H^2(x,y) -z$, and profit $\pi(x,y,z) = z$ (i.e., $\pi(x,y,z) = z -a(y)$ with $a \equiv 0$), where
\begin{equation}\label{distanceh}
\begin{split}
&d_H(x,y)= R\cosh^{-1}\left(\frac{x_0y_0-x_1y_1-\cdots -x_ny_n}{R^2}\right)= R\cosh^{-1}\left[\cosh \frac{r}{R}\cosh \frac{s}{R}-\sinh \frac{r}{R}\sinh\frac{s}{R} M\right],\\
&M=\sum_{i=1}^{n-1}\cos\theta_i \cos\varphi_i\left(\prod_{j=1}^{i-1} \sin\theta_j\sin\varphi_j\right)+ \prod_{j=1}^{n-1} \sin\theta_j\sin\varphi_j,\\
&\text{ here } x=(x_0,x_1,\dots, x_n),~y=(y_0,y_1,\dots, y_n) \in \mathcal{D},\\& x_0 = R\cosh \frac{r}{R},~ x_i = R\sinh \frac{r}{R}\cos\theta_i \Pi_{j=1}^{i-1}\sin \theta_j, \text{ for all } i=1,2,...,n-1, ~x_n = R\sinh \frac{r}{R} \Pi_{j=1}^{n-1}\sin \theta_j;\\
&y_0 = R\cosh \frac{s}{R},~ y_i = R\sinh \frac{s}{R}\cos\varphi_i \Pi_{j=1}^{i-1}\sin \varphi_j, \text{ for all } i=1,2,...,n-1,~ y_n = R\sinh \frac{s}{R} \Pi_{j=1}^{n-1}\sin \varphi_j.
\end{split}
\end{equation} \medskip

Here $\prod$ means the product notation. In order to distinguish it from the profit functional $\pmb \Pi$, equivalently, in this section, we minimize $\mathcal{L} := - \pmb \Pi$. Let $\mu$ be the uniform measure on this hyperbolic disk $\mathcal{D}$. The participation constraint is $u_{\emptyset}(x)= -\frac{1}{2}d_{H}^2(x, y_{\emptyset})$, where $y_{\emptyset} = (R, 0, 0, ..., 0) \in \mathcal{D}$ is the outside option with a fixed price $z_{\emptyset} = 0$.\medskip


Thus, the monopolist problem becomes
\begin{flalign}
	\max\limits_{ u\ge u_\nul \atop \text{$u$ is $G$-convex}} \pmb\Pi(u) &~=\max\limits_{u \ge u_\nul \atop \text{$u$ is $G$-convex}} \int_X \pi(x, \bar{y}_G(x, u(x), Du(x)))  d \mu(x)  \\
	&~= \max\limits_{u \ge u_\nul \atop \text{$u$ is $G$-convex}} \int_X -\frac{1}{2} d_H^2(x, y_G(x, u(x), Du(x))) - u(x)  d \mu(x).\\
	&~= -\min\limits_{u \ge u_\nul \atop \text{$u$ is $G$-convex}} \int_X \frac{1}{2} d_H^2(x, y_G(x, u(x), Du(x))) + u(x)  d \mu(x).\\
(P_5)\hspace{3cm}	&~= -\min\limits_{u \ge u_\nul \atop \text{$u$ is $G$-convex}} \mathcal{L}(u).
\end{flalign}
\medskip

\begin{lemma}\label{dislemma}
	Let $x(t)$ be any curve on $\mathcal{D}$, with $|\dot{x}(t_0)|=1$, and $y$ be any point on $\mathcal{D}$. Suppose $D_td_H(x(t),y)|_{t=t_0}$ exists, then $|D_td_H(x(t),y)|_{t=t_0}|\le 1$. 
\end{lemma}
\begin{proof}
	By the triangle inequality, we have
\begin{flalign*}
		\left|D_td_H(x(t),y)|_{t=t_0}\right| =& \bigg|\lim\limits_{t\rightarrow t_0}\frac{d_H(x(t),y)-d_H(x(t_0),y)}{t-t_0}\bigg|= \lim\limits_{t\rightarrow t_0}\bigg|\frac{d_H(x(t),y)-d_H(x(t_0),y)}{t-t_0}\bigg|\\
		\le&  \lim\limits_{t\rightarrow t_0^+}\frac{d_H(x(t),x(t_0))}{t-t_0} = |\dot{x}(t_0)|=1.
\end{flalign*}
\end{proof}

\begin{corollary}\label{disprop}
	Since  $\left|\frac{\partial x(r, \theta_1,...,\theta_{n-1})}{\partial r}\right|= \lim\limits_{s\rightarrow 0}\frac{d( x(r, \theta_1,...,\theta_{n-1}),  x(r+s, \theta_1,...,\theta_{n-1}))}{s}=\lim\limits_{s\rightarrow 0}\frac{s}{s} = 1$,
	by Lemma (\ref{dislemma}), we have $ \left|D_r d_H(x(r, \theta_1,...,\theta_{n-1}), y)\right|\le 1$, for all $x, y \in \mathcal{D}.$
\end{corollary}
\medskip

The following theorem shows a unique solution $\bar{u}$, with the explicit formula, to the principal-agent problem, on a negative curvature space $\mathcal{D}$. Its proof has three parts. In step 1, we first derive $\bar{u}$ as a local minimizer among the class of $C^1$ radially symmetric functions which are bounded below by the reservation utility, using the Calculus of Variations, then show it is also the unique global minimizer in this class. Then in step 2, 
we prove by definition $\bar{u}$ is $G$-convex. In step 3, we show that $\bar{u}$ is also a minimizer among all the $G$-convex functions. Moreover, the minimizer is unique.\medskip

\begin{theorem}\label{unique_minimizer}
	The program $(P_5)$ has a unique minimizer on $\mathcal{D}$. And 
	\begin{equation*}
	\argmin\limits_{\substack{u\ge -\frac{1}{2}r^2 \\ u \text{ is radially symmetric} \\ u \in C^1(\mathcal{D})}} \mathcal{L}(u)= \argmin\limits_{\substack{u\ge -\frac{1}{2}r^2 \\ u \text{ is radially symmetric} \\ u \text{ is $G$-convex}}} \mathcal{L}(u)=\argmin\limits_{\substack{u\ge u_{\emptyset} \\ u \text{ is $G$-convex}}} \mathcal{L}(u) =: \bar{u}(r)
	\end{equation*}
	where
	\begin{equation*}
	\bar{u}(r) = 
	\begin{dcases}
	-\frac{1}{2} r^2, & 0\le r\le \tilde{r}; \\
	\begin{split}
	&\int_{\tilde{r}}^{r}\sinh^{1-n}\left(\frac{t}{R}\right)\int_{0}^{t}\sinh^{n-1}\left(\frac{\sigma}{R}\right)d\sigma dt \\
	&\hspace{0.3cm}- \int_{0}^{\bar{r}}\sinh^{n-1}\left(\frac{\sigma }{R}\right)d \sigma  \int_{\tilde{r}}^{r}\sinh^{1-n}\left(\frac{t}{R}\right) d t -\frac{1}{2}(\tilde{r})^2, 
	\end{split} 
	& \tilde{r} < r \le \bar{r}.\\
	\end{dcases}
	\end{equation*}
	Here $\tilde{r}$ satisfies 
	\begin{equation*}
	\int_{\bar{r}}^{\tilde{r}}\sinh^{n-1}\left(\frac{\sigma }{R}\right) d\sigma  +\tilde{r}\sinh^{n-1}\left(\frac{\tilde{r}}{R}\right)=0.
	\end{equation*}
\end{theorem}

\begin{proof}
	{\bf Step 1:} Firstly, find the minimizer of $\mathcal{L}(u)$ for all $u$ satisfying $u(r)\ge -\frac{1}{2}r^2$ and $u$ is radially symmetric. Assume $\bar{u} \in C^2$ piecewisely on $\mathcal{D}$ is such a minimizer. For each agent $x$, $\bar{u}(x)= \sup\limits_{y} -\frac{1}{2}d_H^2(x,y)-v(y)$, one can find the optimal $y_G(x, \bar{u}(x), D\bar{u}(x))$ via
	\begin{equation}\label{usym}
	\begin{split}
	D_r\bar{u}(x) = D_r\left(-\frac{1}{2} d_H^2(x, y_G(x, \bar{u}(x), D\bar{u}(x)))\right),\\
	D_{\theta_i}\bar{u}(x) = D_{\theta_i}\left(-\frac{1}{2} d_H^2(x, y_G(x, \bar{u}(x), D\bar{u}(x)))\right).
	\end{split}
	\end{equation}
	From the above equations, we can see that $y_G(x, \bar{u}(x), D\bar{u}(x))$ could be uniquely determined by $x$ and $D\bar{u}(x)$. In this section, we use $y_G(x, D\bar{u}(x))$ to denote $y_G(x, \bar{u}(x), D\bar{u}(x))$.
	
	Since $\bar{u}$ is radially symmetric, thus $D_{\theta_i} \bar{u}(x) =0 $, for all $i=1,2,...,n-1$, and $D_r\bar{u}(x)= \bar{u}'(r)$. From (\ref{usym}), one can derive $\theta_i = \varphi_i$, for all $i=1,2,...,n-1$, and $d_H(x, y_G(x, D\bar{u}(x))) = |r-s|$, $D_rd_H(x,y_G(x, D\bar{u}(x)))=\frac{\sin\frac{r-s}{R}}{|\sin\frac{r-s}{R}|}= \sign(r-s) $, for $x, y_G(x, D\bar{u}(x)) \in \mathcal{D}$ with polar coordinates introduced in (\ref{distanceh}).
	
	 Again from (\ref{usym}), $\bar{u}'(r)+|r-s|\cdot \sign(r-s)=0$ implies $s=r+\bar{u}'(r)$, and $(\bar{u}'(r))^2 =(s-r)^2 = d_H^2(x, y_G(x, D\bar{u}(x)))$. Notice here magnitude of $y_G(x, D\bar{u}(x))$ should be non-negative, so we have constraint $r+\bar{u}'(r) \ge 0$, which will be used later.
	 
	After calculating $y_G(x, D\bar{u}(x))$, one can compute
	\begin{equation*}
	\begin{split}
	\mathcal{L}(\bar{u})=& \int_{\mathcal{D}} \frac{1}{2} d_H^2(x,y_G(x, D\bar{u}(x)))+\bar{u}(x) d\mu(x)\\
	=&\int_{0}^{\bar{r}}\int_{0}^{\pi}\cdots\int_{0}^{\pi}\int_{0}^{2\pi}\left[\frac{(\bar{u}'(r))^2}{2}+\bar{u}(r)\right]R^{n-1}\left(\sinh\frac{r}{R}\right)^{n-1}(\sin \theta_1)^{n-2}\cdots (\sin\theta_{n-2}) d\theta_{n-1}\cdots d\theta_1 dr\\	
	=&C_0\int_{0}^{\bar{r}}\left[\frac{(\bar{u}'(r))^2}{2}+\bar{u}(r)\right] \left(\sinh\frac{r}{R}\right)^{n-1} dr.
	\end{split}
	\end{equation*}
	Here $C_0=\int_{0}^{\pi}\cdots\int_{0}^{\pi}\int_{0}^{2\pi}R^{n-1}(\sin \theta_1)^{n-2}\cdots (\sin\theta_{n-2}) d\theta_{n-1}\cdots d\theta_1$  is a positive constant. 
	
	Since $\bar{u}(r)\ge -\frac{1}{2}r^2$, let $A\in [0,\bar{r}]$, such that $A\times[0,\pi]^{n-2}\times[0,2\pi] = \left\{(r,\theta_1, ...,\theta_{n-1})~|~ \bar{u}(r) = -\frac{1}{2}r^2\right\}$. Define $B =[0,\bar{r}]\setminus A$, so $\bar{u}(r)> -\frac{1}{2}r^2$ on $B$. 
	
	Denote $U_1= \left\{ w\in C^1(\mathcal{D}) ~|~ w \text{ is radially symmetric and } w = 0 \text{ on }  A\times[0,\pi]^{n-2}\times[0,2\pi] \right\}$.
	Since $\bar{u}$ is a minimizer of $\mathcal{L}(u)$, for any $w \in U_1$, one has
	\begin{equation*}
	\begin{split}
	0&=\frac{\partial \mathcal{L}(\bar{u}+\varepsilon w)}{\partial \varepsilon}\bigg|_{\varepsilon=0} \\
	&= C_0\int_B [\bar{u}'(r)w'(r)+w(r)]\left(\sinh\frac{r}{R}\right)^{n-1}dr\\
	&=C_0 \int_B w\left[\left(\sinh\frac{r}{R}\right)^{n-1}-\bar{u}''(r)\left(\sinh\frac{r}{R}\right)^{n-1}- \frac{n-1}{R}\bar{u}'(r)\left(\sinh\frac{r}{R}\right)^{n-2}\cosh\frac{r}{R}\right]dr\\& \ \ \  +C_0 \bar{u}'(r)w(r)\left(\sinh\frac{r}{R}\right)^{n-1}\bigg|_{\partial B}.\\
	\end{split}
	\end{equation*}
	By the fundamental lemma of the Calculus of Variations, and the inequality we derived from non-negativity of the magnitude of $y_G(x, D\bar{u}(x))$, we have following constraints for $\bar{u}$:
	\begin{equation*}
	(ODE) \begin{cases} 
	r+\bar{u}'(r) \ge 0, & (1)\mbox{ on } B; \\ 
	\bar{u}''(r)+\frac{n-1}{R}\cdot \bar{u}'(r)(\coth\frac{r}{R}) -1=0, & (2)\mbox{ on } B\setminus \partial B; \\
	w(r)\bar{u}'(r)(\sinh\frac{r}{R})^{n-1}|_{\partial B} =0,&(3) \mbox{ for all } w\in U_1.\\
	\end{cases}
	\end{equation*}
	The equation (ODE)(2) implies, for all $ r \in B$,
	\begin{equation*}
	\bar{u}(r) = \int_{0}^{r}\left(\sinh\frac{t}{R}\right)^{1-n} \int_{0}^{t} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma  dt + C_1 \int_{0}^{r}\left(\sinh\frac{t}{R}\right)^{1-n} dt +C_2.
	\end{equation*}
	Taking the derivatives of $\bar{u}$, 
	\begin{align*}
	\bar{u}'(r) = \left(\sinh\frac{r}{R}\right)^{1-n} \left[\int_{0}^{r} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma  +C_1\right],\\
	\bar{u}''(r) = 1-\frac{n-1}{R}\cdot \frac{\cosh \frac{r}{R}}{\sinh^n(\frac{r}{R})} \left[ \int_{0}^{r} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma +C_1 \right].
	\end{align*}
	%    \begin{equation*}
	%    	\begin{split}
	%    	\bar{u}'(r) = (\sinh\frac{r}{R})^{1-n} [\int_{0}^{r} (\sinh\frac{\sigma }{R})^{n-1} d\sigma  +C_1],\\
	%    	\bar{u}''(r) = 1-\frac{n-1}{R}\cdot \frac{\cosh \frac{r}{R}}{\sinh^n(\frac{r}{R})}[\int_{0}^{r} (\sinh\frac{\sigma }{R})^{n-1} d\sigma +C_1].\\
	%    	\end{split}
	%    \end{equation*}
	Consider the sign of $C_1$, there are two cases:
	\begin{itemize}
		\item[1.]  If $C_1 \ge 0,$ then $\bar{u}'(r)\ge 0$, which implies $\bar{u}(r)$ is increasing on $B$.
		\item[2.] If $C_1<0$, then $\bar{u}''(r)\ge 1-\frac{n-1}{R}\cdot \frac{\cosh \frac{r}{R}}{\sinh^n(\frac{r}{R})}\int_{0}^{r} (\sinh\frac{\sigma }{R})^{n-1} d\sigma $. Define 
		$$h_1(r):= \frac{R\sinh^n(\frac{r}{R})}{(n-1)\cosh(\frac{r}{R})} - \int_{0}^{r} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma.$$ 
		Then $h_1(0)=0, h_1'(r) = \frac{\sinh^{n-1}(\frac{r}{R})}{(n-1)\cosh^2(\frac{r}{R})}\ge 0$, for all $r\in [0, \bar{r}].$ Thus, $h_1(r) \ge 0$, for all $r\in [0, \bar{r}]$, which implies, $u''(r)\ge 0$, i.e., $\bar{u}$ is convex on $B$.
	\end{itemize}
	
	In either case, $A$ is path-connected, since one cannot join two points on $u_{0}(r) = -\frac{1}{2}r^2$ by either increasing or convex curve above the graph of $u_{0}$.
	
	Assume $A=[\alpha_1, \alpha_2] \neq [0, \bar{r}]$. For the relative position of $A$ and $B$, considering $A \cup B =[0,\bar{r}]$, there are three cases:
	\begin{itemize}
		\item [1.] If $\alpha_1>0, \alpha_2<\bar{r}$, assume $B = B_1 \cup B_2$ with $B_1 = [0, \alpha_1)$ and $B_2 = (\alpha_2, \bar{r}]$. Let $	\bar{u}'(r) = (\sinh\frac{r}{R})^{1-n} [\int_{0}^{r} (\sinh\frac{\sigma }{R})^{n-1} d\sigma  +C_{1_0}]$ on $B_1$ and 	$\bar{u}'(r) = (\sinh\frac{r}{R})^{1-n} [\int_{0}^{r} (\sinh\frac{\sigma }{R})^{n-1} d\sigma  + C_{1_{\bar{r}}}]$ on $B_2$.
		Then by (ODE)(3), one has $\bar{u}'(r)(\sinh\frac{r}{R})^{n-1}|_{r=0}^{\bar{r}} =0$, i.e. $[\int_{0}^{\bar{r}} (\sinh\frac{\sigma }{R})^{n-1} d\sigma  +C_{1_{\bar{r}}}] - C_{1_0} =0$, which implies 
		\begin{flalign}\label{eqn:constant_C_1}
			C_{1_{\bar{r}}} = C_{1_0} -  \int_{0}^{\bar{r}} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma .
		\end{flalign}
		
		Since $\bar{u}(r) \ge u_{0} (r)$ on $B_1$ with equality holds at $r= \alpha_1$, we know $\bar{u}$ could not be increasing on $B_1$. Therefore, by the above discussion, we know $C_{1_0} <0$ and $C_{1_{\bar{r}}}<0$ by \eqref{eqn:constant_C_1}. Thus $\bar{u}$ is convex on $B$. In particular,  $\bar{u}$ is convex and decreasing on $B_1$. Let $\tilde{u} = \bar{u}$ on $A \cup B_2$, and $\tilde{u} = u_{0}$ on $B_1$. Then for any $r\in B_1$, $\tilde{u}(r) - \bar{u}(r) \le 0$ and $0\ge  \tilde{u}'(r)> u_{0}'(\alpha_1) \ge \bar{u}'(\alpha_1) \ge \bar{u}'(r) $. Thus,
		\begin{flalign}
			\mathcal{L}(\tilde{u}) - \mathcal{L}(\bar{u}) = C_0 \int_{0}^{\alpha_1} \left\{\left[\frac{(\tilde{u}'(r))^2}{2} -\frac{(\bar{u}'(r))^2}{2}\right]+\left[\tilde{u}-\bar{u}(r)\right]\right\} \left(\sinh\frac{r}{R}\right)^{n-1} dr < 0.
		\end{flalign}
		Therefore, this case is reduced to the following one where $\alpha_1 =0$.
	
		\item[2.] 	If $\alpha_1=0, \alpha_2\neq\bar{r}$, then by (ODE)(3), one has $\bar{u}'(r)(\sinh\frac{r}{R})^{n-1}|_{r=\bar{r}} =0$, which implies $\bar{u}'(\bar{r}) =0$. Thus, $C_1 = -\int_{0}^{\bar{r}} (\sinh\frac{\sigma }{R})^{n-1} d\sigma $. In this case, $\bar{u}$ is convex and decreasing on $B$.
		
		\item [3.] If $\alpha_1\neq0,\alpha_2=\bar{r}$, then by (ODE)(3), one has $\bar{u}'(r)(\sinh\frac{r}{R})^{n-1}|_{r=0} =0$. That is,  $$\left.\left[\int_{0}^{r} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma  +C_1\right]\right|_{r=0} =0,$$ 
		which implies $C_1 = 0$. In this case, $\bar{u}$ is increasing on $B$. Notice that $\bar{u}(r)\ge -\frac{1}{2} r^2$ with equality holds at $r=\alpha_1$, a contradiction.
	\end{itemize}

	Summing up all the possible cases above, we know there exist $\alpha \in [0, \bar{r}]$, such that $A=[0,\alpha]$, $B=(\alpha, \bar{r}]$. By (ODE)(1), we have for any $r$ on $B$, 
	$$\int_{\bar{r}}^{r}\sinh^{n-1}\left(\frac{\sigma}{R}\right) d\sigma +r\sinh^{n-1}\left(\frac{r}{R}\right)\ge 0.$$ 
	Define 
	$$ h_2(r)=\int_{\bar{r}}^{r}\sinh^{n-1}\left(\frac{\sigma}{R}\right) d\sigma +r\sinh^{n-1}\left(\frac{r}{R}\right).$$ 
	Then $$h_2'(r) = 2\sinh^{n-1}\left(\frac{r}{R}\right) +(n-1)\sinh^{n-2}\left(\frac{r}{R}\right)\left(\cosh\frac{r}{R}\right)\frac{r}{R}>0.$$
	Thus $h_2$ is strictly increasing. Notice that $h_2(\bar{r})>0$, and $h_2(0)<0$. Thus there is a unique solution of $h_2(r)=0$ in $[0, \bar{r}]$, denote it $\tilde{r}$. Then (ODE)(1) implies, $\alpha\ge \tilde{r}$, where $\tilde{r}$ satisfies
	\begin{equation}
	\int_{\bar{r}}^{\tilde{r}}\sinh^{n-1}\left(\frac{\sigma}{R}\right) d\sigma +\tilde{r}\sinh^{n-1}\left(\frac{\tilde{r}}{R}\right)=0.
	\end{equation}
	Since $\bar{u}$ is continuous, at $r=\alpha$, one have 
	\begin{equation*}
	-\frac{1}{2} \alpha^2 = \int_{0}^{\alpha}\left(\sinh\frac{t}{R}\right)^{1-n} \int_{0}^{t} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma  dt  -\int_{0}^{\bar{r}} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma \int_{0}^{\alpha}\left(\sinh\frac{t}{R}\right)^{1-n} dt +C_2.
	\end{equation*}
	This implies 
	\begin{equation*}
	C_2=-\frac{1}{2} \alpha^2 - \int_{0}^{\alpha}\left(\sinh\frac{t}{R}\right)^{1-n} \int_{0}^{t} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma  dt  +\int_{0}^{\bar{r}} \left(\sinh\frac{\sigma }{R}\right)^{n-1} d\sigma \int_{0}^{\alpha}\left(\sinh\frac{t}{R}\right)^{1-n} dt.
	\end{equation*}
	For any given $\alpha \in [\tilde{r},\bar{r}]$, denote 
	\begin{equation*}
	\bar{u}_{\alpha}(r) =
	\begin{dcases}
	-\frac{1}{2} r^2, &0\le r\le \alpha;\\
	\begin{split}
	&\int_{\alpha}^{r}(\sinh\frac{t}{R})^{1-n} \int_{0}^{t} (\sinh\frac{\sigma }{R})^{n-1} d\sigma  dt \\
	&\hspace{0.3cm}-\int_{0}^{\bar{r}} (\sinh\frac{\sigma }{R})^{n-1} d\sigma\int_{\alpha}^{r}(\sinh\frac{t}{R})^{1-n} dt -\frac{1}{2} \alpha^2 ,
	\end{split} & \alpha <r\le \bar{r}.
	\end{dcases}
	\end{equation*}
	
	Define $h_3(\alpha) = \mathcal{L}(\bar{u}_{\alpha})$ for all $\alpha\in [\tilde{r}, \bar{r}]$. 
	
	Then $$h_3'(\alpha) = \frac{C_0}{2}\sinh^{1-n}\left(\frac{\alpha}{R}\right)\left[\alpha\sinh^{n-1}\left(\frac{\alpha}{R}\right)-\int_{\alpha}^{\bar{r}} \sinh^{n-1}\left(\frac{\sigma}{R}\right) d\sigma\right]^2 \ge 0$$ 
	for all $\alpha \in [\tilde{r}, \bar{r}].$
	 
	Define $h_4(\alpha):=\alpha \sinh^{n-1}(\frac{\alpha}{R})-\int_{\alpha}^{\bar{r}} \sinh^{n-1}(\frac{\sigma}{R}) d\sigma$ for all $\alpha \in [\tilde{r}, \bar{r}]$. 
	
	Then $$h_4'(\alpha)= 2\sinh^{n-1}\left(\frac{\alpha}{R}\right) +\frac{n-1}{R}\alpha  \sinh^{n-2}\left(\frac{\alpha}{R}\right) \cosh\left(\frac{\alpha}{R}\right) >0$$  
	on $[\tilde{r}, \bar{r}]$ and $h_4(\tilde{r})=0$, which implies $h_4(\alpha)> 0$, for $\alpha \in (\tilde{r}, \bar{r}]$. 
	
	Thus, $h_3'(\alpha)> 0$, for all $\alpha \in (\tilde{r}, \bar{r}]$, which implies
	\begin{equation*}
	\min\limits_{\alpha\in [\tilde{r}, \bar{r}]} \mathcal{L}(\bar{u}_{\alpha}) = \min\limits_{\alpha\in [\tilde{r}, \bar{r}]} h_3({\alpha}) = h_3(\tilde{r}) = \mathcal{L}(\bar{u}_{\tilde{r}}).
	\end{equation*}
	
	Therefore, $\bar{u}(r) = \bar{u}_{\tilde{r}}(r)$. And
	\begin{equation*}
	\bar{u}(r) =
	\begin{dcases}
	-\frac{1}{2} r^2 & ,0\le r\le \tilde{r}; \\
	\begin{split}
	&\int_{\tilde{r}}^{r}\sinh^{1-n}\left(\frac{t}{R}\right)\int_{0}^{t}\sinh^{n-1}\left(\frac{\sigma}{R}\right)d\sigma dt \\
	&\hspace{0.3cm}- \int_{0}^{\bar{r}}\sinh^{n-1}\left(\frac{\sigma }{R}\right)d \sigma  \int_{\tilde{r}}^{r}\sinh^{1-n}\left(\frac{t}{R}\right) d t -\frac{1}{2}(\tilde{r})^2
	\end{split}
	  & ,\tilde{r} < r \le \bar{r}. \\
	\end{dcases}
	\end{equation*}
	\begin{equation*}
	\bar{u}'(r) =
	\begin{dcases}
	- r & ,0\le r\le \tilde{r}; \\
	\sinh^{1-n}\left(\frac{r}{R}\right)\int_{\bar{r}}^r\sinh^{n-1}\left(\frac{\sigma}{R}\right)d\sigma    & ,\tilde{r} < r \le \bar{r}. \\
	\end{dcases}
	\end{equation*}
	
	
		\begin{equation*}
		\bar{u}''(r) =
		\begin{dcases}
		- 1 & ,0\le r\le \tilde{r}; \\
		1-\frac{(n-1)\cosh\left(\frac{r}{R}\right)\int_{\bar{r}}^r\sinh^{n-1}\left(\frac{\sigma}{R}\right)d\sigma}{R\sinh^n\left(\frac{r}{R}\right)}  & ,\tilde{r} < r \le \bar{r}. \\
		\end{dcases}
		\end{equation*}
	
	
	
	Since $\partial_{-}\bar{u}'(\tilde{r})= \partial_{+}\bar{u}'(\tilde{r})= -\tilde{r}$, thus $\bar{u}'(\tilde{r})$ exists, and $\bar{u}'(\tilde{r})=-\tilde{r}$.
	
	We now show the above $\bar{u}(r)$ is indeed the (global) minimizer, i.e.,
	\begin{equation*}
	\bar{u}(r)=\argmin\limits_{\substack{u\ge -\frac{1}{2}r^2 \\ u \text{ is radially symmetric}\\ u \in C^1(\mathcal{D})}} \mathcal{L}(u).
	\end{equation*}
	Denote $U_2 =\left\{ w\in C^1(\mathcal{D}) ~|~ w \text{ is symmetric and } w\ge 0 \text{ on }  A\times[0,\pi]^{n-2}\times[0,2\pi] \right\}$.
	For any $w \in U_2$, 
	\begin{flalign}\label{difference_L_1}
	\begin{split}
		&\ \mathcal{L}(\bar{u}+w)-\mathcal{L}(\bar{u})\\
		= &\ C_0 \int_{0}^{\bar{r}} \left[\frac{1}{2}(\bar{u}'(r)+w'(r))^2-\frac{1}{2}(\bar{u}'(r))^2+w\right]\sinh^{n-1}\left(\frac{r}{R}\right) dr
	\end{split}
	\end{flalign}
	Drop a non-negative term with integrand $\frac{1}{2}(w'(r))^2$, plug in $\bar{u}'$ on $A \times[0,\pi]^{n-2}\times[0,2\pi] $ and use the integrate by parts formula, one has
	\begin{equation*}
	\begin{split}
	\eqref{difference_L_1} \ge &\ C_0\int_{0}^{\bar{r}} [\bar{u}'(r)w'(r)+w]\sinh^{n-1}\left(\frac{r}{R}\right) dr.\\
	=&\ C_0\int_{0}^{\tilde{r}} [-rw'(r)+w]\sinh^{n-1}\left(\frac{r}{R}\right) dr + C_0\int_{\tilde{r}}^{\bar{r}} [\bar{u}'(r)w'(r)+w]\sinh^{n-1}\left(\frac{r}{R}\right) dr\\
	=&\ C_0\int_{0}^{\tilde{r}} w\left[2\sinh^{n-1}\left(\frac{r}{R}\right)+\frac{n-1}{R}r\sinh^{n-2}\left(\frac{r}{R}\right)\cosh\left(\frac{r}{R}\right)\right]dr-\left.C_0rw(r)\sinh^{n-1}\left(\frac{r}{R}\right)\right|_0^{\tilde{r}}\\
	&\ \ + C_0\int_{\tilde{r}}^{\bar{r}} w\left[(1-\bar{u}''(r))\sinh^{n-1}\left(\frac{r}{R}\right)-\frac{n-1}{R}\bar{u}'(r)\sinh^{n-2}\left(\frac{r}{R}\right)\cosh\left(\frac{r}{R}\right)\right]dr\\
	&\ \ +\left. C_0\bar{u}'(r)w(r)\sinh^{n-1}\left(\frac{r}{R}\right)\right|_{\tilde{r}}^{\bar{r}}
	\end{split}
	\end{equation*}
	By equation (ODE)(2), the term 
	$$ C_0\int_{\tilde{r}}^{\bar{r}} w\left[(1-\bar{u}''(r))\sinh^{n-1}\left(\frac{r}{R}\right)-\frac{n-1}{R}\bar{u}'(r)\sinh^{n-2}\left(\frac{r}{R}\right)\cosh\left(\frac{r}{R}\right)\right]dr$$
	vanishes.
	Drop the term with non-negative integrand 
	$$ C_0\int_{0}^{\tilde{r}} w\left[2\sinh^{n-1}\left(\frac{r}{R}\right)+\frac{n-1}{R}r\sinh^{n-2}\left(\frac{r}{R}\right)\cosh\left(\frac{r}{R}\right)\right]dr.$$ 
	
	Thus,
	\begin{flalign*}
		\eqref{difference_L_1} \ge &\left. -C_0 r w(r)\sinh^{n-1}\left(\frac{r}{R}\right)\right|_0^{\tilde{r}}+\left.C_0\bar{u}'(r)w(r)\sinh^{n-1}\left(\frac{r}{R}\right)\right|_{\tilde{r}}^{\bar{r}}\ \ \ \ \\ %(\text{the first integrand is non-negative and the second one is zero by (ODE)(2)})\\
		=& \ 0.
	\end{flalign*}
	The last equality holds because $\bar{u}'(\tilde{r}) = -\tilde{r}$ and $\bar{u}'(\bar{r}) = 0$.
	
	In addition, if $\mathcal{L}(\bar{u}+w)-\mathcal{L}(\bar{u})=0$, from above inequalities and since $w \in C^1$, we have $w'(r)=0$ on $\mathcal{D}$, i.e., $w=C_3$, for some non-negative constant $C_3$. Then 
	$$0= \mathcal{L}(\bar{u}+w)-\mathcal{L}(\bar{u})\ge  C_0\int_{0}^{\bar{r}} [\bar{u}'(r)w'(r)+w]\sinh^{n-1}\left(\frac{r}{R}\right) dr =  C_0C_3\int_{0}^{\bar{r}} \sinh^{n-1}\left(\frac{r}{R}\right) dr\ge 0.$$
	 Since both $C_0$ and $\int_{0}^{\bar{r}} \sinh^{n-1}(\frac{r}{R}) dr$ are positive, we have $C_3 = 0$, i.e. $w\equiv 0 $ on $\mathcal{D}$. Therefore, $\bar{u}$ is the unique minimizer.\medskip
	
	{\bf Step 2:} To check that $\bar{u}(x)$ is $G$-convex, it is equivalent to prove $\bar{u}(x)$ is $b$-convex in the sense of~\cite{FigalliKimMcCann11}, or equivalently $-\bar{u}(x)$ is $(-b)$-concave in the sense of Definition \ref{(-bar{G})-convexity}, where $b(x,y):=-\frac{1}{2}d_H^2(x,y)$. That is, we need to show $\bar{u}(x) = -((-\bar{u})^{(-b)^*})^{(-b)}(x)$, for all $x \in \mathcal{D}$. Denote $\psi(y) = -(-\bar{u})^{(-b)^*}(y)$, and $\phi(x) = -(-\psi)^{(-b)}(x)$. Then it is equivalent to show $\bar{u}(x) = \phi(x)$, where
	\begin{flalign*}
		\phi(x) &~= \sup\limits_{y} b(x,y)%-\frac{1}{2}d_H^2(x,y) 
		- \psi(y), \\
		\text{and } \psi(y) &~= \sup\limits_{x} b(x,y) %-\frac{1}{2}d_H^2(x,y)
		- \bar{u}(x).
	\end{flalign*}
	
	By definition, we have
	\begin{equation*}
	\psi(y)=\sup\limits_{x} -\frac{1}{2}d_H^2(x,y) - \bar{u}(x)
	\end{equation*}
	From equation \eqref{distanceh}, we know that $|M|\le 1$ and $M=1$ holds when  $\theta_i= \varphi_i$ for each $i = 1,2,..., n-1$. Thus,
	\begin{flalign*}
	\psi(y)=\sup\limits_{r} -\frac{1}{2}(r-s)^2 - \bar{u}(r)
	\end{flalign*}
	
	For each $s\in [0,\bar{r}]$, define $h_5^s(r):= -\frac{1}{2}(r-s)^2 - \bar{u}(r)$. Then 
	\begin{equation*}
	(h_5^s)'(r) =
	\begin{dcases}
	s & ,0\le r\le \tilde{r}; \\
	s-r-\sinh^{1-n}\left(\frac{r}{R}\right)\int_{\bar{r}}^r\sinh^{n-1}\left(\frac{\sigma}{R}\right)d\sigma    & ,\tilde{r} < r \le \bar{r}. \\
	\end{dcases}
	\end{equation*}
	\begin{equation*}
	(h_5^s)''(r)= 
	\begin{dcases}
	0 & ,0\le r\le \tilde{r}; \\
	-1-\bar{u}''(r)<-1    & ,\tilde{r} < r \le \bar{r}. \\
	\end{dcases}
	\end{equation*}
	Thus, $h_5^s(r)$ is concave on $[0,\bar{r}]$ and strictly concave on $[\tilde{r}, \bar{r}]$. Since $\bar{u}'$ is continuous, thus $(h_5^s)'(r)$ is continuous. Notice $(h_5^s)'(\tilde{r})=s>0$ and $(h_5^s)'(\bar{r})<0$. Therefore, for each $s\in [0,\bar{r}]$, $(h_5^s)'(\beta) =0$ has exactly one solution on $[0, \bar{r}]$, which is located on $[\tilde{r}, \bar{r}]$ and takes the maximum value of $h_5^s$.
	
	Let $h_6(s)$ be the unique solution of $(h_5^s)'(\beta) =0$, for all $s\in [0,\bar{r}]$.
	
	Then $h_6(s)\in [\tilde{r}, \bar{r}]$, and $h_6(0) = \tilde{r}$, $h_6(\bar{r})= \bar{r}$. 
	
	Since $(h_5^s)'(s)>0 = (h_5^s)'(h_6(s))$ and $(h_5^s)'$ is strictly decreasing on $[\tilde{r}, \bar{r}]$, we have $h_6(s)>s$.
	
	For any $0\le s_1<s_2\le \bar{r}$, $ (h_5^{s_2})'(h_6(s_2)) =0 =  (h_5^{s_1})'(h_6(s_1)) < (h_5^{s_2})'(h_6(s_1))$, thus $h_6(s_1)<h_6(s_2)$, i.e., $h_6$ is strictly increasing. By the Implicit Function Theorem, one has $h_6 \in C^1$. Thus, $h_6' >0$.
	Here, denote
	\begin{equation*}
	\bar{u}(r)= 
	\begin{cases}
	u_1(r) & ,0\le r\le \tilde{r}; \\
	u_2(r) & ,\tilde{r} < r \le \bar{r}. \\
	\end{cases}
	\end{equation*}
	Then $\psi(y)=\sup\limits_{r}h_5^s(r) = h_5^s(h_6(s)) = -\frac{1}{2}(h_6(s)-s)^2 - u_2(h_6(s))$.
	\begin{equation*}
	\begin{split}
	\phi(x) =&\sup\limits_{y} -\frac{1}{2}d_H^2(x,y) - \psi(y)\\
	=&\sup\limits_{s} -\frac{1}{2}(r-s)^2+ \frac{1}{2}(h_6(s)-s)^2 + u_2(h_6(s)) \ \ \\ %(\text{ because } |M|\le 1, \text{ and } M=1 \text{ when } \theta_i= \varphi_i, \text{ for all } i =1,2,..., n-1.)\\
	\end{split}
	\end{equation*}
	For each $r\in [0,\bar{r}]$, define $h_7^r(s):=-\frac{1}{2}(r-s)^2+ \frac{1}{2}(h_6(s)-s)^2 + u_2(h_6(s))$. Then
	\begin{align*}
	(h_7^r)'(s)=r-h_6(s),\\
	(h_7^r)''(s)=-(h_6)'(s) < 0.
	\end{align*}
	
	Plugging in $s=0,\bar{r}$, we have $(h_7^r)'(0)=r-\tilde{r}$, $(h_7^r)'(\bar{r})=r-\bar{r}\le 0$. Therefore,
	\begin{itemize}
		\item [1.]  For  $r \in [0, \tilde{r}]$, $(h_7^r)'(s)< (h_7^r)'(0) \le 0$, then we have  $\phi(x) =\sup\limits_{s} (h_7^r)(s) = (h_7^r)(0)=-\frac{1}{2}r^2 = u_1(r)$;
		\item [2.] For  $r \in [ \tilde{r}, \bar{r}]$, since $(h_7^r)'(0) \ge 0, (h_7^r)'(\bar{r}) \le 0$, then we have  $\phi(x) =\sup\limits_{s} (h_7^r)(s) = (h_7^r)((h_6^{-1})(r))= u_2(r)$.
	\end{itemize}
	
%	\begin{equation*}
%	\begin{split}
%	&1. \mbox{ For } r \in [0, \tilde{r}], (h_7^r)'(s)< (h_7^r)'(0) \le 0, \text{ then we have } \phi(x) =\sup\limits_{s} (h_7^r)(s) = (h_7^r)(0)=-\frac{1}{2}r^2 = u_1(r);\\
%	&2.  \mbox{ For } r \in [ \tilde{r}, \bar{r}], \mbox{since} (h_7^r)'(0) \ge 0, (h_7^r)'(\bar{r}) \le 0, \text{ then we have } \phi(x) =\sup\limits_{s} (h_7^r)(s) = (h_7^r)((h_6^{-1})(r))= u_2(r).\\
%	\end{split}
%	\end{equation*}
	Thus, $\phi(x) = \bar{u}(x)$, which implies $\bar{u}$ is $G$-convex. So,
	\begin{equation*}
	\bar{u}(r)= \argmin\limits_{\substack{u\ge -\frac{1}{2}r^2 \\ u \text{ is radially symmetric} \\ u \text{ is $G$-convex}}} \mathcal{L}(u).
	\end{equation*}\medskip
	
	{\bf Step 3:} We are going to show
	\begin{equation*}
	\bar{u}=\argmin\limits_{\substack{u\ge u_{\emptyset} \\ u \text{ is $G$-convex}}} \mathcal{L}(u) .
	\end{equation*}

	Suppose $u$ is any $G$-convex function (not necessarily radially symmetric), then there exists a function $v$, such that $u(x)=\max\limits_{y} b(x,y)-v(y)$. Thus, 
	$$D_ru(x) = D_rb(x,y_G(x,Du(x))) = -d_H(x,y_G(x,Du(x)))\cdot D_rd_H(x,y_G(x,Du(x))),$$ 
	where $y_G(x, Du(x))=\argmax\limits_{y} b(x,y)-v(y)$. By Corollary ($\ref{disprop}$), we have
	\begin{equation}\label{inequality_b_y_G}
	-b(x,y_G(x,Du(x))) = \frac{1}{2}d_H^2(x,y_G(x,Du(x))) =\frac{|D_r u(x)|^2}{2|D_rd_H(x,y_G(x,Du(x)))|^2} \ge \frac{|D_r u(x)|^2}{2}.  
	\end{equation}
	Denote $U_3 = \left\{w: \mathcal{D}\rightarrow \R ~|~ \bar{u}+w \text{ is } G \text{-convex}, \text{and } w\ge 0 \text{ on }  A\times[0,\pi]^{n-2}\times[0, 2\pi] \right\}$.
	For any $w\in U_3$, we have
	\begin{flalign}\label{difference_L_2}
	\begin{split}
	&\mathcal{L}(\bar{u}+w)-\mathcal{L}(\bar{u}) \\
	=&\int_{\mathcal{D}}\!\!{} \left[-b(x, y_G(x,D\bar{u}(x)+Dw(x)))+b(x,y_G(x, D\bar{u}(x)))+w(x)\right]d\mu(x).
	\end{split}
	\end{flalign}
	Similar to \eqref{inequality_b_y_G}, one has $$-b(x, y_G(x,D\bar{u}(x)+Dw(x))) \ge \frac{1}{2}|D_r\bar{u}+D_rw|^2$$ 
	 $$\text{and }\ \ b(x,y_G(x, D\bar{u}(x))) = -\frac{1}{2}|\bar{u}'(r)|^2.$$ Thus, 
	\begin{flalign*}
	\eqref{difference_L_2}\ge& \int_{\mathcal{D}} \!\!{} \left[\frac{1}{2}|D_r\bar{u}+D_rw|^2-\frac{1}{2}|\bar{u}'(r)|^2 +w(x)\right] d\mu(x).
	\end{flalign*}
	Simplify the right hand side, drop a non-negative term with integrand $\frac{1}{2}|D_rw|^2$ and change to polar coordinates, then plug in $\bar{u}'$ on $A\times [0,\pi]^{n-2}\times[0,2\pi]$ and $B\times [0,\pi]^{n-2}\times[0,2\pi]$, separately. Denote $$d\Theta = R^{n-1}\sin^{n-2}\theta_1\sin^{n-3}\theta_2 \cdots \sin\theta_{n-2}  d\theta_1 d\theta_2 \cdots d\theta_{n-1}.$$
	Thus 
	\begin{flalign*}
	\eqref{difference_L_2} \ge& \int_{\mathcal{D}} [\bar{u}'(r)\cdot D_rw+w]R^{n-1}\sinh^{n-1}\left(\frac{r}{R}\right)\sin^{n-2}\theta_1\sin^{n-3}\theta_2 \cdots \sin\theta_{n-2} dr d\theta_1 d\theta_2 \cdots d\theta_{n-1} \\
	=&\int_{A\times [0,\pi]^{n-2}\times[0,2\pi]} \left[-r\cdot\sinh^{n-1}\left(\frac{r}{R}\right)\cdot D_rw+w\cdot \sinh^{n-1}\left(\frac{r}{R}\right)\right] dr d\Theta\\
	& \hspace{0.5cm} +\int_{B\times [0,\pi]^{n-2}\times[0,2\pi]} \left[\int_{\bar{r}}^r\sinh^{n-1}\left(\frac{t}{R}\right) dt\cdot D_rw+w\cdot \sinh^{n-1}\left(\frac{r}{R}\right)\right] dr d\Theta.
	\end{flalign*}
	Use the integration by parts formula, then drop the term with non-negative integrand $$\int_{ [0,\pi]^{n-2}\times[0,2\pi]} \int_{0}^{\tilde{r}}w\left[2\sinh^{n-1}\left(\frac{r}{R}\right)+\frac{n-1}{R}r\sinh^{n-2}\left(\frac{r}{R}\right)\cosh\left(\frac{r}{R}\right)\right]dr d\Theta.$$ 
	Thus,
	\begin{flalign*}
	 \eqref{difference_L_2} \ge&\int_{ [0,\pi]^{n-2}\times[0,2\pi]} \Bigg\{\int_{0}^{\tilde{r}}w\left[2\sinh^{n-1}\left(\frac{r}{R}\right)+\frac{n-1}{R}r\sinh^{n-2}\left(\frac{r}{R}\right)\cosh\left(\frac{r}{R}\right)\right] dr\\
	 &\hspace{2.7cm}-\left.\left[r\sinh^{n-1}\left(\frac{r}{R}\right)\cdot w\right]\right|_{r=0}^{\tilde{r}}\Bigg\}   d\Theta\\
	 &\hspace{0.5cm}+\int_{ [0,\pi]^{n-2}\times[0,2\pi]} \left.\left[\int_{\bar{r}}^{r}\sinh^{n-1}\left(\frac{t}{R}\right)dt \cdot w\right] \right|_{r=\tilde{r}}^{\bar{r}}   d\Theta\\
	 \ge &\int_{[0,\pi]^{n-2}\times [0, 2\pi]}\left[-\tilde{r}\sinh^{n-1}\left(\frac{\tilde{r}}{R}\right) -\int_{\bar{r}}^{\tilde{r}}\sinh^{n-1}\left(\frac{t}{R}\right)dt\right] \cdot w(\tilde{r}, \theta_1,...,\theta_{n-1}) d\Theta\\
	 =& \ \ 0.
	\end{flalign*}
	The last integral equals to 0 by the definition of $\tilde{r}$.
	Therefore, $\mathcal{L}(\bar{u}+w)\ge \mathcal{L}(\bar{u})$ for any  $w\in U_3$. For any $G$-convex $u\ge u_{\emptyset}$,  $u-\bar{u}\in U_3$. So
	\begin{equation*}
	\bar{u}\in \argmin\limits_{\substack{u\ge u_{\emptyset} \\ u \text{ is $G$-convex}}} \mathcal{L}(u) .
	\end{equation*}
	
	If, in addition,  $\mathcal{L}(\bar{u}+w)-\mathcal{L}(\bar{u})=0$, then the above inequalities must be equalities. Thus $D_rw(x)=0$, for almost every $x \in \mathcal{D}$. Since both $\bar{u}+w$ and $\bar{u}$ are $G$-convex, $w \in C^{1,1}$, thus $D_r w(x)\equiv 0$, for all $x\in \mathcal{D}$. So, one can write $w(x)= w(\theta_1, ..., \theta_{n-1})$. Since for $x\in A\times [0,\pi]^{n-1}\times[0,2\pi]$, $w(x)\ge 0$, we have $w\ge 0$ on $\mathcal{D}$. Then from the above inequalities, we get 
	\begin{equation*}
	0=\mathcal{L}(\bar{u}+w)-\mathcal{L}(\bar{u})\ge \int_{\mathcal{D}} w(x)R^{n-1}\sinh^{n-1}(\frac{r}{R})\sin^{n-2}\theta_1\cdots \sin\theta_{n-2} dr d\theta_1 \cdots d\theta_{n-1}\ge 0
	\end{equation*} 
	Thus $ w(x)R^{n-1}\sinh^{n-1}(\frac{r}{R})\sin^{n-2}\theta_1\cdots \sin\theta_{n-2} =0$, for almost every $ x \in \mathcal{D}$. This implies $w \equiv 0 $ on $\mathcal{D}$. So $\bar{u}$ is the unique minimizer, i.e. 
	\begin{equation*}
	\bar{u}= \argmin\limits_{\substack{u\ge u_{\emptyset} \\ u \text{ is $G$-convex}}} \mathcal{L}(u).
	\end{equation*}




\end{proof}



	\begin{remark}
	We also have uniqueness results with different explicit solutions on $\mathbf{S}^n$ and $\R^n$, where the uniqueness is also ensured by Figalli-Kim-McCann\cite{FigalliKimMcCann11}. Moreover, the solutions on $\mathbf{S}^n$, $\HH^n$ converge to those on $\R^n$, as curvatures go to 0.
	 
	The unique minimizer of the principal-agent problem on $\R^n$ is given by
	\begin{flalign*}
			\bar{u}_{\R^n}(r) = 
			\begin{dcases}
			-\frac{1}{2} r^2 & ,0\le r\le \tilde{r}_{\R^n}; \\
			\frac{(\bar{r})^n}{n(n-2)}r^{2-n} + \frac{r^2}{2n} - \frac{(\bar{r})^2}{2(n-2)(n+1)^{\frac{2-n}{n}}} & ,\tilde{r}_{\R^n} < r \le \bar{r}.
			\end{dcases}
	\end{flalign*}
		Here $\tilde{r}_{\R^n} = \frac{\bar{r}}{(n+1)^{\frac{1}{n}}} $. 
	
	Moreover, the unique minimizer on  $\mathbf{S}^n$ is given by
		\begin{equation*}
		\bar{u}_{\mathbf{S}^n}(r) = 
		\begin{dcases}
		-\frac{1}{2} r^2 & ,0\le r\le \tilde{r}_{\mathbf{S}^n}; \\
		\begin{split}
		&\int_{\tilde{r}_{\mathbf{S}^n}}^{r}\sin^{1-n}\left(\frac{t}{R}\right)\int_{0}^{t}\sin^{n-1}\left(\frac{\sigma}{R}\right)d\sigma dt \\
		&\hspace{0.3cm}- \int_{0}^{\bar{r}}\sin^{n-1}\left(\frac{\sigma }{R}\right)d \sigma  \int_{\tilde{r}_{\mathbf{S}^n}}^{r}\sin^{1-n}\left(\frac{t}{R}\right) d t -\frac{(\tilde{r}_{\mathbf{S}^n})^2}{2} 
		\end{split}
		 & ,\tilde{r}_{\mathbf{S}^n} < r \le \bar{r}.\\
		\end{dcases}
		\end{equation*}
	Here $\tilde{r}_{\mathbf{S}^n}$ satisfies 
	\begin{equation*}
		\int_{\bar{r}}^{\tilde{r}_{\mathbf{S}^n}}\sin^{n-1}\left(\frac{\sigma }{R}\right) d\sigma  +\tilde{r}_{\mathbf{S}^n}\sin^{n-1}\left(\frac{\tilde{r}_{\mathbf{S}^n}}{R}\right)=0.
		\end{equation*}
		
	The proofs are similar to that of Theorem \ref{unique_minimizer}.

\end{remark}
\newpage

\section{Convexity results on several examples for the non-quasilinear case}\label{section:example_convexity}



We  close with several examples,  which are established by computing two derivatives
of $\pi(x,y_t,z_t)$ along an arbitrary $G$-segment $t\in [0,1] \longmapsto (x,y_t,z_t)$.
These computations are tedious but straightforward.\medskip

For specific non-quasilinear agent preferences,  we use the explicit expression in Lemma \ref{LemmaProfitConcavity} for the 
desired second derivative to establish the following examples,  which assume the principal is indifferent to
whom she transacts business with and that her preferences depend linearly on payments.  
These examples give conditions under which the principal's program inherits concavity or convexity 
from the agents' price sensitivity.
Although the resulting conditions appear complicated,  they illustrate
the subtle interplay between the preferences of the agent and the principal for products in the first example,
and between the preferences of the agents for products as opposed to prices in the second. \medskip



\begin{example}[Nonlinear yet homogeneous sensitivity of agents to prices]
	\label{general example1}
	Take $\pi(x, y, z) =z- a(y)$, $G(x, y, z) = b(x,y)-f(z)$, satisfying \Gzero-\Gsix,  $G \in C^3(cl(X\times Y \times Z)
	)$, $\pi \in C^2(cl(X\times Y \times Z)
	)$, and assume $\bar{z}<+\infty$.
	
	1. If $f(z)$ is convex [respectively concave] in $cl(Z)$, then $\pmb \Pi(u)$ is concave [respectively convex] for all $\mu\ll \mathcal{L}^m$ if and only if there exists $\varepsilon \ge 0$ such that each $(x,y,z) \in X \times Y\times Z$ and $\xi \in \R^{n}$ satisfy 
	\begin{equation}\label{robert1}
	\begin{split}
	\pm \Bigg\{a_{kj}(y)-\frac{b_{,kj}(x,y)}{f'(z)}+\Big(\frac{b_{,l}(x,y)}{f'(z)}- a_l(y)\Big)b^{i,l}(x,y) b_{i,kj}(x,y)\Bigg\} \xi^{k}\xi^{j} \
	\ge  \varepsilon \mid \xi\mid ^2.
	\end{split}
	\end{equation}
	
	2. In addition, $\pmb \Pi(u)$ is uniformly concave [respectively uniformly convex] on $W^{1,2}(X,d\mu)$  uniformly for all $\mu\ll \mathcal{L}^m$ if and only if $\pm f''> 0$ and  \eqref{robert1} holds with $\varepsilon >0$.
\end{example}

\begin{proof}%[Proof of Example \ref{general example1}]
	From Lemma \ref{LemmaProfitConcavity}, $\pmb \Pi(u)$ is concave for all $\mu\ll \mathcal{L}^m$ if and only if $(\pi_{,\bar{k}\bar{j}}- \pi_{,\bar{l}} \bar{G}^{\bar i,\bar l}\bar{G}_{\bar{i},\bar{k}\bar{j}})\big|_{x_0=-1}$ is non-positive definite, and uniformly concave uniformly for all $\mu\ll \mathcal{L}^m$ if and only if this matrix is uniform negative definite.
	
	In this example, we have $\pi(x,y,z)= z-a(y)$, $\bar{G}(x,x_0, y,z) = x_{0} G(x,y,z)$ $ = x_{0}(b(x,y)-f(z))$. Thus, 
	\begin{flalign*}
	\pi_{,\bar{k}\bar{j}}= \begin{pmatrix}
	-a_{kj} & \mathbf{0}\\
	\mathbf{0} & 0\\
	\end{pmatrix}, \ \ 
	\pi_{,\bar{l}}= (-a_{l}, 1), \ \ 
	\bar{G}_{\bar{i},\bar{l}}\big|_{x_0=-1} = \begin{pmatrix}
	-b_{i,l} & \mathbf{0}\\
	b_{,l} & -f'(z)\\
	\end{pmatrix}.\\	    
	\end{flalign*}
	
	By \Gfour, $f'(z) >0$ for all $z\in cl(Z)$. By \Gsix, since $\bar{G}_{\bar{i},\bar{l}}\big|_{x_0=-1}$ has the full rank, the matrix $(b_{i,l})$ also has its full rank. Taking $b^{i,l}$ as its left inverse, we have
	\begin{flalign*}
	\left.\bar{G}^{\bar i,\bar l}\right|_{x_0=-1} = \begin{pmatrix}
	-b^{i,l} & \mathbf{0}\\
	-\frac{b_{,l}b^{i,l}}{f'(z)} & \frac{1}{-f'(z)}\\
	\end{pmatrix}, \ \ 
	\left.\bar{G}_{\bar{i},\bar{k}\bar{j}}\right|_{x_0=-1} =\begin{pmatrix}
	-b_{i,k\bar{j}}& \mathbf{0}  \\
	b_{,k\bar{j}} & (-f'(z))_{\bar{j}}\\
	\end{pmatrix}.
	\end{flalign*}
	Therefore, 
	\begin{flalign*}
	&\left.\left(\pi_{,\bar{k}\bar{j}} - \pi_{,\bar{l}}\bar{G}^{\bar i,\bar l} \bar{G}_{\bar{i},\bar{k}\bar{j}}\right)\right|_{x_0=-1} \\
	&=\begin{pmatrix}
	-a_{kj} &\mathbf{0} \\
	\mathbf{0} &  0\\
	\end{pmatrix} - \begin{pmatrix}
	(-a_{l}b^{i,l}+\frac{b_{,l}}{f'(z)}b^{i,l}) b_{i,k\bar{j}}-\frac{b_{,k\bar{j}}}{f'(z)},\frac{(f'(z))_{\bar{j}}}{f'(z)}
	\end{pmatrix}\\
	&= - \begin{pmatrix}
	a_{kj}+(-a_{l}+\frac{b_{,l}}{f'(z)})b^{i,l} b_{i,kj}-\frac{b_{,kj}}{f'(z)} & \mathbf{0}\\
	\mathbf{0} & \frac{f''(z)}{f'(z)}\\
	\end{pmatrix}.
	\end{flalign*}
	
	
	Since \Gfour\ and $f$ is convex, we have $f'(z) >0$ and $f''(z)\ge 0$, for all $z\in cl(Z)$. Thus, $\pi_{,\bar{k}\bar{j}}-\pi_{,\bar{l}}\bar{G}^{\bar i,\bar l}\bar{G}_{\bar{i},\bar{k}\bar{j}}$ is non-positive definite if and only if $a_{kj}+(-a_{l}+\frac{b_{,l}}{f'(z)})b^{i,l} b_{i,kj}-\frac{b_{,kj}}{f'(z)}$ is non-negative definite, i.e., 
	there exist $\varepsilon \ge 0$ such that each $(x,y,z) \in X \times Y\times Z$ and $\xi \in \R^{n}$ satisfy 
	\begin{equation*} \Bigg\{a_{kj}(y)-\frac{b_{,kj}(x,y)}{f'(z)}+\Big(\frac{b_{,l}(x,y)}{f'(z)}- a_l(y)\Big)b^{i,l}(x,y) b_{i,kj}(x,y)\Bigg\} \xi^{k}\xi^{j}\ 
	\ge  \varepsilon \mid \xi\mid ^2.
	\end{equation*}
	
	In addition, $\pi_{,\bar{k}\bar{j}}-\pi_{,\bar{l}}\bar{G}^{\bar i,\bar l}\bar{G}_{\bar{i},\bar{k}\bar{j}}$ is uniform negative definite if and only if $f''>0$ and $\varepsilon>0$, which is equivalent to that $\pmb \Pi(u)$ is uniformly concave uniformly for all $\mu\ll \mathcal{L}^m$. Similarly, one can show equivalent conditions for $\pmb \Pi(u)$ being convex or uniformly convex.
\end{proof}

Although 
the next two examples are not completely general, they have the following economic interpretation. The same selling price impacts utility differently for different types of agents. In other words, it models the situation where agents have different sensitivities to the same price.  In Example $\ref{general example2}$,  the principal's utility is linear and depends exclusively on her revenue, which is a simple special case of Example $\ref{general example3}$.
\medskip

\begin{example}[Inhomogeneous sensitivity of agents to prices, zero cost]\label{general example2}
	Take $\pi(x, y, z) =z$,  $G(x,y,z)$ $= b(x,y)-f(x,z)$,  satisfying \Gzero-\Gsix,  $G \in C^3(cl(X\times Y \times Z)
	)$, $\pi \in C^2(cl(X\times Y \times Z)
	)$, and assume $\bar{z}<+\infty$. Suppose $D_{x,y}b(x,y)$ has full rank for each $(x,y) \in X\times Y$, and denote its left inverse $b^{i,l}(x,y).$
	
	1. If $(x,y,z)\longmapsto h(x,y,z):=f(x,z)-b_{,l}(x,y)b^{i,l}(x,y)f_{i,}(x,z)$ is strictly increasing and convex [respectively concave] with respect to $z$, then $\pmb \Pi(u)$ is concave [respectively convex]  for all $\mu\ll \mathcal{L}^m$ if and only if there exists $\varepsilon \ge 0$ such that each $(x,y) \in X \times Y$ and $\xi \in \R^{n}$ satisfy
	\begin{equation}\label{robert2}
	\pm \Big\{-b_{,kj}(x,y)+b_{,l}(x,y)b^{i,l}(x,y) b_{i,kj}(x,y)\Big\} \xi^{k}\xi^{j} \ge  \varepsilon \mid \xi\mid ^2.
	\end{equation}
	
	2. In addition, $\pmb \Pi(u)$ is uniformly concave [respectively uniformly convex] on $W^{1,2}(X,d\mu)$ uniformly for all $\mu\ll \mathcal{L}^m$ if and only  if $\pm h_{zz}> 0$ and \eqref{robert2} holds with $\varepsilon >0$.
	
\end{example}


\begin{example}[Inhomogeneous sensitivity of agents to prices]\label{general example3}
	Take $\pi(x, y,$ $ z) =z-a(y)$,  $G(x,y,z)= b(x,y)-f(x,z)$,  satisfying \Gzero-\Gsix,  $G \in C^3(cl(X\times Y \times Z)
	)$, $\pi \in C^2(cl(X\times Y \times Z)
	)$, and assume $\bar{z}<+\infty$. Suppose $D_{x,y}b(x,y)$ has full rank for each $(x,y) \in X\times Y$, and $1- (f_{z})^{-1}b_{,\beta}b^{\alpha,\beta}f_{\alpha,z} \ne 0$, for all $(x, y,z) \in X\times Y\times Z$.
	
	1. If $(x,y,z)\longmapsto h(x,y,z):=a_{l}b^{i,l}f_{i,zz}+\frac{(a_{\beta}b^{\alpha,\beta}f_{\alpha,z}-1)(b_{,l}b^{i,l}f_{i,zz}-f_{zz})}{f_{z} -b_{,\beta}b^{\alpha,\beta}f_{\alpha,z}} \ge 0 ~[\le 0]$ , then $\pmb \Pi(u)$ is concave  [respectively convex] for all $\mu\ll \mathcal{L}^m$ if and only if there exists $\varepsilon \ge 0$ such that each $(x,y,z) \in X \times Y\times Z$ and $\xi \in \R^{n}$ satisfy 
	\begin{equation}\label{robert3}
	\begin{split}
	\pm \Bigg\{&~a_{kj} -a_{l}b^{i,l}b_{i,kj} +\frac{1-a_{\beta}b^{\alpha,\beta}f_{\alpha,z}}
	{1- (f_{z})^{-1}b_{,\beta}b^{\alpha,\beta}f_{\alpha,z}}
	\Big(-\frac{b_{,kj}}{f_z}+\frac{b_{,l}}{f_z}b^{i,l} b_{i,kj}\Big) \Bigg\}\xi^{k}\xi^{j} \
	\ge  \varepsilon \mid \xi\mid ^2.
	\end{split}
	\end{equation}
	
	2. If in addition, $\pmb \Pi(u)$ is uniformly concave [respectively uniformly convex] on $W^{1,2}(X,d\mu)$ uniformly for all $\mu\ll \mathcal{L}^m$ if and only if $\pm h>0$ and  \eqref{robert3} holds with $\varepsilon>0$.
\end{example}

\begin{proof}%[Proof of Example \ref{general example3}]
	Similar to the proof of Example \ref{general example1},  $\pmb \Pi(u)$ is concave for all $\mu\ll \mathcal{L}^m$ if and only if $(\pi_{,\bar{k}\bar{j}}-\pi_{,\bar{l}}\bar{G}^{\bar i,\bar l}\bar{G}_{\bar{i},\bar{k}\bar{j}})$ is non-positive definite, and  uniformly concave uniformly for all $\mu\ll \mathcal{L}^m$ if and only if this tensor is uniform negative definite.
	
	Since $D_{x,y}b(x,y)$ has full rank for each $(x,y) \in X\times Y$, and for all $(x, y,z) \in X\times Y\times Z$, $1- (f_{z})^{-1}b_{,\beta}b^{\alpha,\beta}f_{\alpha,z} \ne 0$, for $\pi(x,y,z) = z-a(y)$, $\bar{G}(x,x_0, y, z) = x_0(b(x,y)-f(x,z))$, we have 
	\begin{flalign*}
	&-(\pi_{,\bar{k}\bar{j}} - \pi_{,\bar{l}}\bar{G}^{\bar i,\bar l} \bar{G}_{\bar{i},\bar{k}\bar{j}}) \\
	& = \begin{pmatrix}
	\begin{split}
	&a_{kj} -a_{l}b^{i,l}b_{i,kj} +\frac{(a_{\beta}b^{\alpha,\beta}f_{\alpha,z}-1)(b_{,kj}-b_{,l}b^{i,l} b_{i,kj})}{f_{z} -b_{,\beta}b^{\alpha,\beta}f_{\alpha,z}}
	\end{split} & \begin{split}
	\mathbf{0}
	\end{split}\vspace{0.5cm}\\
	\begin{split}
	\mathbf{0}
	\end{split} & \begin{split}
	h(x,y,z)	
	\end{split}\\
	\end{pmatrix},
	\end{flalign*}
	where $h(x,y,z) = a_{l}b^{i,l}f_{i,zz}+\frac{(a_{\beta}b^{\alpha,\beta}f_{\alpha,z}-1)(b_{,l}b^{i,l}f_{i,zz}-f_{zz})}{f_{z} -b_{,\beta}b^{\alpha,\beta}f_{\alpha,z}}$. Since $h(x,y,z)\ge 0$, then $(\pi_{,\bar{k}\bar{j}}-\pi_{,\bar{l}}\bar{G}^{\bar i,\bar l}\bar{G}_{\bar{i},\bar{k}\bar{j}})$ is non-positive definite if and only if there exist $\varepsilon \ge 0$ such that each $(x,y,z) \in X \times Y\times Z$ and $\xi \in \R^{n}$ satisfy 
	\begin{equation*}
	\left\{a_{kj} -a_{l}b^{i,l}b_{i,kj}+\frac{(a_{\beta}b^{\alpha,\beta}f_{\alpha,z}-1)(b_{,kj}-b_{,l}b^{i,l} b_{i,kj})}{f_{z} -b_{,\beta}b^{\alpha,\beta}f_{\alpha,z}}\right\} \xi^{k}\xi^{j} \ 
	\ge  \varepsilon \mid \xi\mid ^2.
	\end{equation*} 
	
	In addition, $\pmb \Pi(u)$ is uniformly concave uniformly for all $\mu\ll \mathcal{L}^m$ if and only if $h>0$ and $\varepsilon>0$.
\end{proof}



Example \ref{general example 4} asserts the concavity of monopolist's maximization in the zero-sum setting,
where the agent's utilities are relatively general but the principal's profit is extremely special. Also, more non-quasilinear examples could be discovered by applying Lemma $\ref{LemmaProfitConcavity}$.\medskip



\begin{example}[Zero sum transactions]\label{general example 4}
	Take $\pi(x, y, z) = -G(x,y,z)$, satisfying \Gzero-\Gfive\ and $\mu\ll \mathcal{L}^m$, which means the monopolist's profit in each transaction coincides exactly with the 
	agent's loss. From $(\ref{$G$-segment})$, since $G$ is linear on $G$-segments, we know $\pmb \Pi(u)$ is linear.
\end{example}			




